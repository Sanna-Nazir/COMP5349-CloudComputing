{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Q1.Part-2 : TRAINING DATASET VOCABULARY EXPLORATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING THE SPARK SESSION\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType,IntegerType, FloatType\n",
    "from pyspark.sql.functions import concat,lit\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import avg, round\n",
    "\n",
    "\n",
    "# INITIATE SPARK SESSION\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark Q1 Part2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# CREATING DATA PATH\n",
    "training_path = \"s3://comp5349-snaz3253/Assignment_Data/train.tsv\"\n",
    "\n",
    "# stopwords list\n",
    "stop_words = list(stopwords.words('english')) \n",
    "\n",
    "\n",
    "# CREATING DATAFRAMES\n",
    "train_df = spark.read.csv(training_path,sep='\\t',header=True,inferSchema=\"true\")\n",
    "\n",
    "# EXTRACTING USEFUL COUMNS AND CONCATENATING THE SENTENCES\n",
    "train_df = train_df.select(['genre',(concat(train_df.sentence1, lit(\" \"), train_df.sentence2).alias('joined'))])\n",
    "\n",
    "# CONVERT TO RDDS FOR TOKENIZATION\n",
    "train_rdd = train_df.rdd.map(list)\n",
    "\n",
    "\n",
    "#creating mapper functions\n",
    "\n",
    "def removePunctuationsFunct(record):\n",
    "    list_punct=list(string.punctuation)\n",
    "    x = str(record[1])\n",
    "    for punct in list_punct:\n",
    "        if punct in x:\n",
    "            x = x.replace(punct, '')\n",
    "    return (record[0],x)\n",
    "\n",
    "def tokens_with_stopwords(record):\n",
    "    words = word_tokenize(record[1])\n",
    "    new_words= [word.lower() for word in words if word.isalpha()]\n",
    "    return (record[0], new_words)\n",
    "\n",
    "\n",
    "def tokens_without_stopwords(record):\n",
    "    words = word_tokenize(record[1])\n",
    "    new_words= [word.lower() for word in words if word.isalpha()]\n",
    "    filtered_words = [w for w in new_words if not w in stop_words] \n",
    "    return (record[0], filtered_words)\n",
    "\n",
    "\n",
    "#remove punctuation from sentences\n",
    "new_train_rdd = train_rdd.map(removePunctuationsFunct)\n",
    "\n",
    "#extract tokens\n",
    "train_tokenized_with_stopwords = new_train_rdd.map(tokens_with_stopwords)\n",
    "train_tokenized_without_stopwords = new_train_rdd.map(tokens_without_stopwords)\n",
    "\n",
    "#CONVERT BACK TO DF\n",
    "df_with_stopwords = spark.createDataFrame(train_tokenized_with_stopwords)\n",
    "df_without_stopwords = spark.createDataFrame(train_tokenized_without_stopwords)\n",
    "\n",
    "\n",
    "#rename columns\n",
    "df_with_stopwords = df_with_stopwords.withColumnRenamed('_1','genre')\n",
    "df_with_stopwords = df_with_stopwords.withColumnRenamed('_2','BOW')\n",
    "\n",
    "df_without_stopwords = df_without_stopwords.withColumnRenamed('_1','genre')\n",
    "df_without_stopwords = df_without_stopwords.withColumnRenamed('_2','BOW')\n",
    "\n",
    "\n",
    "# exploding the dataframe to form separate rows for each word\n",
    "from pyspark.sql.functions import split, explode,col\n",
    "df_exploded_with_stopwords = df_with_stopwords.withColumn('BOW',explode('BOW'))\n",
    "df_exploded_without_stopwords = df_without_stopwords.withColumn('BOW',explode('BOW'))\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import col, countDistinct\n",
    "result = df_exploded_with_stopwords.groupBy(\"BOW\").agg(countDistinct(\"genre\").alias(\"Common_Genre_Count\"))\n",
    "result_without_stopwords = df_exploded_without_stopwords.groupBy(\"BOW\").agg(countDistinct(\"genre\").alias(\"Common_Genre_Count\"))\n",
    "\n",
    "result.cache()\n",
    "result_without_stopwords.cache()\n",
    "\n",
    "\n",
    "# GENERATE AND DISPLAY THE STATISTICS\n",
    "\n",
    "# Counting the total number of unique words existing in the training data corpus\n",
    "total_words_with_stopwords = result.count()\n",
    "total_words_without_stopwords = result_without_stopwords.count()\n",
    "\n",
    "\n",
    "# dropping the words column (since we only require the number of words existing in genre combinations, not the words)\n",
    "# generate the count of words for each genre combination and rename the column\n",
    "x = result.drop('BOW').groupBy('Common_Genre_Count').count().withColumnRenamed('count','Number_of_Words')\n",
    "y = result_without_stopwords.drop('BOW').groupBy('Common_Genre_Count').count().withColumnRenamed('count','Number_of_Words')\n",
    "\n",
    "# generate new column that holds percentages of the words existing in various genre combinations\n",
    "final_df_with_stopwords = x.withColumn(\"Percentages\", round(F.col(\"Number_of_Words\")/total_words_with_stopwords*100, 2))\n",
    "final_df_without_stopwords = y.withColumn(\"Percentages\", round(F.col(\"Number_of_Words\")/total_words_without_stopwords*100, 2))\n",
    "\n",
    "\n",
    "# print out the required results for first requirement\n",
    "print('REQUIREMENT 1: WITHOUT REMOVING STOP WORDS\\n\\n')\n",
    "print('The number of unique words in the training data corpus are: ')\n",
    "print(total_words_with_stopwords)\n",
    "print('\\nThe number of words existing in various genre combinations with their percentages are displayed as under:\\n')\n",
    "final_df_with_stopwords.sort('Common_Genre_Count').show()\n",
    "print('***********************************************************************************************************')\n",
    "\n",
    "\n",
    "# print out the required results for second requirement\n",
    "print('REQUIREMENT 2: AFTER REMOVING STOP WORDS\\n\\n')\n",
    "print('The number of unique words in the training data corpus are: ')\n",
    "print(total_words_without_stopwords)\n",
    "print('\\nThe number of words existing in various genre combinations with their percentages are displayed as under:\\n')\n",
    "final_df_without_stopwords.sort('Common_Genre_Count').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
