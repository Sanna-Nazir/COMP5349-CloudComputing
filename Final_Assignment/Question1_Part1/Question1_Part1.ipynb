{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1.Part-1: MATCH AND MISMATCH VOCABULARY EXPLORATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b51b66710e44392b8faacf3871ad2f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tError sending http request and maximum retry encountered..\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "# DEFINING THE SPARK SESSION AND IMPORTING PACKAGES\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType,IntegerType, FloatType\n",
    "from pyspark.sql.functions import concat,lit\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "\n",
    "# INITIATE THE SPARK SESSION\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark Q1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# DEFINE FILE PATHS\n",
    "dev_matched = \"s3://comp5349-snaz3253/Assignment_Data/dev_matched.tsv\"\n",
    "test_matched = \"s3://comp5349-snaz3253/Assignment_Data/test_matched.tsv\"\n",
    "dev_mis_matched = \"s3://comp5349-snaz3253/Assignment_Data/dev_mismatched.tsv\"\n",
    "test_mis_matched = \"s3://comp5349-snaz3253/Assignment_Data/test_mismatched.tsv\"\n",
    "\n",
    "\n",
    "# IMPORTING DATA INTO DATAFRAMES\n",
    "dev_match_df = spark.read.csv(dev_matched,sep='\\t',header=True,inferSchema=\"true\")\n",
    "test_match_df = spark.read.csv(test_matched,sep='\\t',header=True,inferSchema=\"true\")\n",
    "dev_mis_match_df = spark.read.csv(dev_mis_matched,sep='\\t',header=True,inferSchema=\"true\")\n",
    "test_mis_match_df = spark.read.csv(test_mis_matched,sep='\\t',header=True,inferSchema=\"true\")\n",
    "\n",
    "# EXTRACTING USEFUL COLUMNS FROM DATAFRAMES\n",
    "dev_match_df = dev_match_df.select(['sentence1','sentence2'])\n",
    "test_match_df = test_match_df.select(['sentence1','sentence2'])\n",
    "dev_mis_match_df = dev_mis_match_df.select(['sentence1','sentence2'])\n",
    "test_mis_match_df = test_mis_match_df.select(['sentence1','sentence2'])\n",
    "\n",
    "# MERGING DATAFRAMES TO CREATE COMPLETE MATCH AND MISMATCH SETS\n",
    "matched = dev_match_df.union(test_match_df)\n",
    "mis_matched = dev_mis_match_df.union(test_mis_match_df)\n",
    "\n",
    "# CONCATENATING THE TWO SENTENCES TO FORM A SINGLE SET\n",
    "match_df = matched.select(concat(matched.sentence1, lit(\" \"), matched.sentence2).alias('joined'))\n",
    "mis_matched_df = mis_matched.select(concat(mis_matched.sentence1, lit(\" \"), mis_matched.sentence2))\n",
    "\n",
    "\n",
    "# CONVERTING TO RDDS NOW\n",
    "matched_rdd = match_df.rdd.map(list)\n",
    "mis_matched_rdd = mis_matched_df.rdd.map(list)\n",
    "\n",
    "\n",
    "# DEFINING MAPPER FUNCTIONS FOR PERFORMING TRANSFORMATIONS\n",
    "\n",
    "def punctuation_remover(record):\n",
    "    list_punct=list(string.punctuation)\n",
    "    x = str(record)\n",
    "    for punct in list_punct:\n",
    "        if punct in x:\n",
    "            x = x.replace(punct, '')\n",
    "    return (x)\n",
    "\n",
    "def custom_tokenizer(string):\n",
    "    words = word_tokenize(string)\n",
    "    list_of_words = [(word.lower(),1) for word in words if word.isalpha()]\n",
    "    return list_of_words\n",
    "\n",
    "def pairFlagToWord(record, flag):\n",
    "    return (record[0], flag)\n",
    "\n",
    "\n",
    "# TRANSFORMING RDDS NOW\n",
    "\n",
    "## Remove punctuations from sentences\n",
    "matched_rdd_punct = matched_rdd.map(punctuation_remover)\n",
    "mis_matched_rdd_punct = mis_matched_rdd.map(punctuation_remover)\n",
    "\n",
    "## Extract tokens from sentences\n",
    "matched_rdd_tokenized = matched_rdd_punct.flatMap(custom_tokenizer)\n",
    "mis_matched_rdd_tokenized = mis_matched_rdd_punct.flatMap(custom_tokenizer)\n",
    "\n",
    "## Aggregate the tokens to give unqiue tokens and their number\n",
    "matched_unique_tokens = matched_rdd_tokenized.aggregateByKey(0, (lambda x, y : x + y), (lambda x, y : x + y), 1)\n",
    "mis_matched_unique_tokens = mis_matched_rdd_tokenized.aggregateByKey(0, (lambda x, y : x + y), (lambda x, y : x + y), 1)\n",
    "\n",
    "## Flagging the words to show which set of vocabulary they belong to\n",
    "flagged_match_tokens = matched_unique_tokens.map(lambda row: pairFlagToWord(row,\"match_vocabulary\"))\n",
    "flagged_mismatch_tokens = mis_matched_unique_tokens.map(lambda row: pairFlagToWord(row,\"mismatch_vocabulary\"))\n",
    "\n",
    "# COUNTING COMMON AND UNIQUE WORDS \n",
    "\n",
    "## Join the entire match and mismatch corpuses\n",
    "Complete_Corpus = flagged_match_tokens.fullOuterJoin(flagged_mismatch_tokens)\n",
    "Complete_Corpus.cache()\n",
    "\n",
    "## Convert flagged df to dataframe\n",
    "Complete_Corpus_df = spark.createDataFrame(Complete_Corpus).withColumnRenamed('_1','Word').withColumnRenamed('_2','Required_Combination')\n",
    "\n",
    "#print out required statistics\n",
    "print(\"The required statistics for Question 1 for the matched and mismatched corpora are as below: \\n\")\n",
    "print('Total unique words in the combined corpus is:', Complete_Corpus_df.count())\n",
    "\n",
    "\n",
    "print('\\n\\n***************************************************************************\\n\\n')\n",
    "\n",
    "print('The various combinations of words across match and mismatch vocabulary sets are given as under:')\n",
    "Complete_Corpus_df.groupBy('Required_Combination').count().show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
