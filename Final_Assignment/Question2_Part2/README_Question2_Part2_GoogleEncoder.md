
# README FOR SENTENCE VECTOR EXPLORATION OF TRAINING DATA CORPUS USING GOOGLE UNIVERSAL SENTENCE ENCODER (USE)#


#FOLLOW THE BELOW INSTRUCTIONS TO RUN THE ATTACHED CODE:#

## SECTION 1: UPLOAD FILES

##STEP 1: 
Login to your AWS account and navigate to S3 services.
Upload the following files to your appropriate S3 bucket/folder:
	1. Upload the required data file (training data file) to your appropriate S3 bucket. 
	2. Also upload the week 10 bootstrap script provided (to enable NLTK and Tensorflow Hub usage).


## SECTION 2: CREATING CLUSTER

##STEP 2:
Now proceed to EMR dashboard (Under services), click ‘Create Cluster’ button to start creating a cluster.You will be presented with a “quick options” page, click go to advanced options link right next to Quick Options title at the top of the page to enter the advanced options selection page.

##STEP 3:
In the first part, Software and steps configuration: select EMR release 5.29 from the drop down list. Then click Hadoop 2.8.5,Spark 2.4, Zeppelin 0.8.2 Tensorflow 1.14.0 and Livy 0.6.0 to include the three components.

##STEP 4:
In Edit Software Settings section, paste the following configuration into the provided text box. This is the configuration to enable maximum resource allocation:

[
{
"Classification": "spark",
"Properties": {
"maximizeResourceAllocation": "true"
}
}
]

##STEP 5:
In the Hardware configuration section: select to use 1 master node and 4 core/slave nodes. Please select m4.xlarge instance type for all of the nodes. 

##STEP 6:
In general cluster settings: change the cluster name as usual. Further, expand the Bootstrap Actions options at the bottom of the page. In Add bootstrap action drop down list, select “Custom action”, then click Configure and Add. This will bring out a window allowing you to select the script wk10_bootstrap.sh you uploaded to S3.

##STEP 7:
In the Security section: select your unique private key (generated by AWS) in the EC2 key pair selection box.
Click “Launch cluster” after all settings are updated. Bootstrapping and launching may take more than 10 minutes for the cluster to launch. Wait for the cluster state to go to waiting. 

## SECTION 3: CHANGE CLUSTER/SPARK CONFIGURATIONS: (VERY IMPORTANT)

##STEP 8:
Once the cluster has been launched, start an SSH session to the EMR cluster using PUTTY and your AWS Private Key. 
We need to change some configurations in the hadoop conf file as well as the spark conf file.

FOR THE HADOOP CONF CHANGES:

    1. cd /etc/hadoop/conf
    2. sudo vim yarn-site.xml
    3. Insert this with same alignment before last property:

  	<property>
    		<name>yarn.nodemanager.vmem-check-enabled</name>
    		<value>false</value>
  	</property>

	<property>
    		<name>yarn.nodemanager.pmem-check-enabled</name>
    		<value>false</value>
  	</property>


FOR THE SPARK CONF CHANGES:

    1. cd /etc/spark/conf
    2. sudo vim spark-defaults.conf
    3. Hash this line:
	#spark.dynamicAllocation.enabled  true
 
    4. Change the following configurations to given values:

	spark.network.timeout          			10000s
	spark.executor.heartbeatInterval           	80s
	spark.executor.instances         		7
	spark.executor.cores             		3
	spark.driver.memory              		7372M
	spark.executor.memory            		7372M
	spark.yarn.executor.memoryOverhead  		819M
	spark.yarn.driver.memoryOverhead  		819M
	spark.default.parallelism        		70
	spark.emr.maximizeResourceAllocation 		true


## SECTION 4: INSTALL/ADD REQUIRED PACKAGES TO CLUSTER

## STEP 9:
Within the SSH session for the hadoop cluster, install the Sklearn and Seaborn Packages for data visualization purposes (later in the code file).
You may use this command:
	sudo pip-3.6 install --quiet sklearn
	sudo pip-3.6 install --quiet seaborn

## SECTION 4: CREATING EMR NOTEBOOK

##STEP 10:
After changing all the configurations of the cluster as mentioned above, go back to the dashboard and click Notebooks on the left hand side menu list. This will bring you to the noteboook page. Click “Create notebook” button to start the creation process.

##STEP 11:
Enter a notebook name such as “MovieData_Summary”. In “clusters” selection, click “Choose an existing cluster” then click choose button. This will bring up a pop up window showing all eligible clusters. The cluster you have created should be listed there. Select it then click “Choose cluster” button to choose
it. 

##STEP 12:
In “Notebook location” selection, click “Choose an existing S3 location in us-east-1”, this will bring up a window showing your S3 buckets, choose the required bucket. 

##STEP 13:
Click Create notebook button to create it. This will brings you to the notebook detail page. You will see the notebook status changes from “Starting” to “Pending” to “Ready”.


## SECTION 5: UPLOAD AND RUN REQUIRED CODE FILE

##STEP 14:
Once the notebook is ready, Click "Open in Jupyter." This will take you to the Jupyter Home page.

##STEP 15:
From the top right corner, click the upload option and upload the code file (.ipynb) attached with this readme (Question2_Part2_GoogleEncoder.ipynb).

##STEP 16: 
Once uploaded, open the file. Change the kernel to "PySpark."
Change the data path files within the defined variables to your S3 file locations.
And run all the cells.  