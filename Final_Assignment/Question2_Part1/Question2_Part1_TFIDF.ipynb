{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2.Part1: K-Means clustering using TF-IDF Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f28c220d67ca4d899d81fe6fcc9d3eec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>5</td><td>application_1589965105790_0006</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-31-75.ec2.internal:20888/proxy/application_1589965105790_0006/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-26-86.ec2.internal:8042/node/containerlogs/container_1589965105790_0006_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dba4e0826a54ff28299862b2845cb06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import basic spark session and requirements\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# importing ML functionalities\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# importing functions for concatenation of sentences\n",
    "from pyspark.sql.functions import concat,lit\n",
    "\n",
    "# importing libraries for performing tokenization and punctuation removal\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "\n",
    "# importing TFIDF\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "\n",
    "# starting spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Assignment Q2 Solution\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "#importing required dataset\n",
    "data_path = \"s3://akum44880/assignmen/train.tsv\"\n",
    "df = spark.read.csv(data_path,header=True,sep='\\t')\n",
    "\n",
    "\n",
    "#selecting only required columns\n",
    "data_df = df.select(['genre','sentence1','sentence2'])\n",
    "\n",
    "#concatenating the dataset to form just one column of words\n",
    "df = data_df.select(['genre',concat(data_df.sentence1, lit(\" \"), data_df.sentence2).alias('joined')])\n",
    "\n",
    "# converting df to RDD now\n",
    "df_rdd = df.rdd\n",
    "\n",
    "#creating mapper functions\n",
    "\n",
    "def removePunctuationsFunct(record):\n",
    "    list_punct=list(string.punctuation)\n",
    "    x = str(record[1])\n",
    "    for punct in list_punct:\n",
    "        if punct in x:\n",
    "            x = x.replace(punct, '')\n",
    "    return (record[0],x)\n",
    "\n",
    "def extract_tokens(record):\n",
    "    words = word_tokenize(record[1])\n",
    "    new_words= [word.lower() for word in words if word.isalpha()]\n",
    "    #filtered_words = [(w,1) for w in new_words if not w in stop_words]\n",
    "    return (record[0],new_words)\n",
    "\n",
    "\n",
    "#remove punctuation from sentences\n",
    "new_rdd = df_rdd.map(removePunctuationsFunct)\n",
    "\n",
    "# now tokenized the bag of sentences to give bag of words for each genre\n",
    "tokenized = new_rdd.map(extract_tokens)\n",
    "\n",
    "\n",
    "#convert tokenized rdd back to dataframe\n",
    "tokenized_df = spark.createDataFrame(tokenized)\n",
    "\n",
    "#rename columns\n",
    "tokenized_df = tokenized_df.withColumnRenamed('_1','genre')\n",
    "tokenized_df = tokenized_df.withColumnRenamed('_2','BOW')\n",
    "\n",
    "\n",
    "#Using TFIDF to encode words\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"BOW\", outputCol=\"rawFeatures\", numFeatures=300)\n",
    "featurizedData = hashingTF.transform(tokenized_df)\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "data_embedding = rescaledData.select(\"genre\", \"features\")\n",
    "\n",
    "\n",
    "data_embedding.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attemtping k-means clustering on the encoded dataset now\n",
    "\n",
    "kmeans = KMeans(featuresCol='features',k=5).setSeed(56)\n",
    "model=kmeans.fit(data_embedding)\n",
    "predictions = model.transform(data_embedding)\n",
    "centers = model.clusterCenters()\n",
    "\n",
    "data_embedding.unpersist()\n",
    "#create final dataframe with true labels (genre) and predicted labels (prediction)\n",
    "final_predicted = predictions.select('genre','prediction')\n",
    "#final_predicted.count()\n",
    "\n",
    "\n",
    "#generate counts for each\n",
    "result = final_predicted.groupBy(\"genre\",\"prediction\").count().orderBy(\"prediction\",\"count\")\n",
    "result.cache()\n",
    "\n",
    "\n",
    "#intialising labels to store each prediction label with actual value count for mapping purpose\n",
    "\n",
    "list_of_labels=[]\n",
    "\n",
    "#storing the actual genre and count for prediction 0\n",
    "list_of_labels.append( result.filter(result.prediction==0).orderBy('count',ascending=False).collect())\n",
    "#storing the actual genre and count for prediction 1\n",
    "list_of_labels.append( result.filter(result.prediction==1).orderBy('count',ascending=False).collect())\n",
    "#storing the actual genre and count for prediction 2\n",
    "list_of_labels.append( result.filter(result.prediction==2).orderBy('count',ascending=False).collect())\n",
    "#storing the actual genre and count for prediction 3\n",
    "list_of_labels.append( result.filter(result.prediction==3).orderBy('count',ascending=False).collect())\n",
    "#storing the actual genre and count for prediction 4\n",
    "list_of_labels.append( result.filter(result.prediction==4).orderBy('count',ascending=False).collect())\n",
    "\n",
    "\n",
    "\n",
    "#if this class is already assigned to some other predicted label, we map the next available actual class to the label\n",
    "\n",
    "def cmax(z):\n",
    "    for i in z:\n",
    "        maxg = i['genre']\n",
    "        if(flag[maxg]==0):\n",
    "            flag[maxg]=1\n",
    "            return maxg\n",
    "        \n",
    "#dictionary to keep a check whether the actual class has been assigned to a predicted label or not\n",
    "flag={'fiction':0,'slate':0,'travel':0,'telephone':0,'government':0} \n",
    "\n",
    "#Enabler to map the actual class to predicted label\n",
    "enabler={'fiction':0,'slate':0,'travel':0,'telephone':0,'government':0} \n",
    "\n",
    "mapped_clusters=[]\n",
    "i=0\n",
    "\n",
    "for l in list_of_labels:\n",
    "    g=cmax(l)\n",
    "    mapped_clusters.append(g) #storing all the actual classes in order of their predicted labels\n",
    "    enabler[g]=i #mapping the actual class to their predicted label\n",
    "    i=i+1\n",
    "    \n",
    "# replace the actual numerical cluster labels with mapped categorical labels\n",
    "y = final_predicted.withColumn('prediction', result.prediction.cast('string'))\n",
    "y = y.na.replace(['0', '1','2','3','4'], mapped_clusters, 'prediction')\n",
    "\n",
    "# import libraries for visualization and confusion matrix\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# create the matrix from a numpy array of the generated results\n",
    "x = np.array(y.collect())\n",
    "cf_matrix = confusion_matrix(x[:,1], x[:,0])\n",
    "\n",
    "# create custom confusion matrix with percentages for our requirement\n",
    "def my_confusion_matrix(array):\n",
    "    a = []\n",
    "    row = array.shape[0]\n",
    "    column = array.shape[1]\n",
    "    for i in range(row):\n",
    "        l = []\n",
    "        for j in range(column):\n",
    "            x = (array[i][j]/sum(array[i]))*100\n",
    "            l.append(round(x,2))\n",
    "        a.append(l)\n",
    "    return a\n",
    "\n",
    "cd = my_confusion_matrix(cf_matrix)\n",
    "\n",
    "\n",
    "# visualize the final confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "sns.heatmap(cd, annot=True, fmt='.2f', cmap='Blues', xticklabels=predicted, yticklabels=predicted)\n",
    "plt.ylabel('ACTUAL LABELS')\n",
    "plt.xlabel('PREDICTED LABELS')\n",
    "plt.show(block=False)\n",
    "\n",
    "%matplot plt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
