
# README FOR SENTENCE VECTOR EXPLORATION OF TRAINING DATA CORPUS USING TF-IDF ENCODING#


#FOLLOW THE BELOW INSTRUCTIONS TO RUN THE ATTACHED CODE:#

## SECTION 1: UPLOAD FILES

##STEP 1: 
Login to your AWS account and navigate to S3 services.
Upload the following files to your appropriate S3 bucket/folder:
	1. Upload the required data file (training data file) to your appropriate S3 bucket. 
	2. Also upload the week 10 bootstrap script provided (to enable NLTK and Tensorflow Hub usage).


## SECTION 2: CREATING CLUSTER

##STEP 2:
Now proceed to EMR dashboard (Under services), click ‘Create Cluster’ button to start creating a cluster.You will be presented with a “quick options” page, click go to advanced options link right next to Quick Options title at the top of the page to enter the advanced options selection page.

##STEP 3:
In the first part, Software and steps configuration: select EMR release 5.29 from the drop down list. Then click Hadoop 2.8.5,Spark 2.4, Zeppelin 0.8.2 Tensorflow 1.14.0 and Livy 0.6.0 to include the three components.

##STEP 4:
In Edit Software Settings section, paste the following configuration into the provided text box. This is the configuration to enable maximum resource allocation:

[
{
"Classification": "spark",
"Properties": {
"maximizeResourceAllocation": "true"
}
}
]

##STEP 5:
In the Hardware configuration section: select to use 1 master node and 4 core/slave nodes. Please select m4.xlarge instance type for all of the nodes. 

##STEP 6:
In general cluster settings: change the cluster name as usual. Further, expand the Bootstrap Actions options at the bottom of the page. In Add bootstrap action drop down list, select “Custom action”, then click Configure and Add. This will bring out a window allowing you to select the script wk10_bootstrap.sh you uploaded to S3.

##STEP 7:
In the Security section: select your unique private key (generated by AWS) in the EC2 key pair selection box.

##STEP 8:
Click “Launch cluster” after all settings are updated. Bootstrapping and launching may take more than 10 minutes for the cluster to launch. Wait for the cluster state to go to waiting. 

## SECTION 3: INSTALL/ADD REQUIRED PACKAGES TO CLUSTER

## STEP 9:
After the cluster is launched, start an SSH session for the hadoop cluster and install the Sklearn and Seaborn Packages for data visualization purposes (later in the code file).
You may use this command:
	sudo pip-3.6 install --quiet sklearn
	sudo pip-3.6 install --quiet seaborn


## SECTION 4: CREATING EMR NOTEBOOK

##STEP 9:
After creating a cluster, go back to the dashboard and click Notebooks on the left hand side menu list. This will bring you to the noteboook page. Click “Create notebook” button to start the creation process.

##STEP 10:
Enter a notebook name such as “MovieData_Summary”. In “clusters” selection, click “Choose an existing cluster” then click choose button. This will bring up a pop up window showing all eligible clusters. The cluster you have created should be listed there. Select it then click “Choose cluster” button to choose
it. 

##STEP 11:
In “Notebook location” selection, click “Choose an existing S3 location in us-east-1”, this will bring up a window showing your S3 buckets, choose the required bucket. 

##STEP 12:
Click Create notebook button to create it. This will brings you to the notebook detail page. You will see the notebook status changes from “Starting” to “Pending” to “Ready”.


## SECTION 5: UPLOAD AND RUN REQUIRED CODE FILE

##STEP 13:
Once the notebook is ready, Click "Open in Jupyter." This will take you to the Jupyter Home page.

##STEP 14:
From the top right corner, click the upload option and upload the code file (.ipynb) attached with this readme (Question2_Part1_TFIDF.ipynb).

##STEP 15: 
Once uploaded, open the file. Change the kernel to "PySpark."
Change the data path files within the defined variables to your S3 file locations.
And run all the cells.  